\chapter{Singularity}
\label{ch:singularity}

\section{What is Singularity?}

Singularity is an open-source computer program that performs operating-system-level virtualization (also known as containerisation).

One of the main uses of Singularity is to bring containers and reproducibility to
scientific computing and the high-performance computing (HPC) world.
Using Singularity containers, developers can work in reproducible environments of their choosing and design,
and these complete environments can easily be copied and executed on other platforms.

For more general information about the use of Singularity, please see the official documentation at \url{https://www.sylabs.io/docs/}.

This documentation only covers aspects of using Singularity on the \hpcInfra infrastructure.

\section{Restrictions on image location}

Some restrictions have been put in place on the use of Singularity. This is mainly done
for performance reasons and to avoid that the use of Singularity impacts other users on the system.

The Singularity image file must be located on either one of the scratch filesystems,
the local disk of the workernode you are using or \lstinline|/dev/shm|. The centrally provided \lstinline|singularity|
command will refuse to run using images that are located elsewhere, in particular on the \lstinline|$VSC_HOME|,
\lstinline|/apps| or \lstinline|$VSC_DATA| filesystems.

In addition, this implies that running containers images provided via a URL
(e.g., \lstinline|shub://...| or \lstinline|docker://...|) will not work.

If these limitations are a problem for you, please let us know via \hpcinfo.

\section{Available filesystems}

All HPC-UGent shared filesystems will be readily available in a Singularity container,
including the home, data and scratch filesystems, and they will be accessible via the
familiar \lstinline|$VSC_HOME|, \lstinline|$VSC_DATA*| and \lstinline|$VSC_SCRATCH*| environment variables.

\section{Singularity Images}

\subsection{Creating Singularity images}

Creating new Singularity images or converting Docker images, by default, requires admin privileges,
which is obviously not available on the \hpcInfra infrastructure. However,
if you use the \lstinline|--fakeroot| option, you can make new Singularity images or convert 
Docker images. 

When you make Singularity or convert Docker images you have some restrictions.

\begin{itemize}
    \item You can only create Singularity images on RHEL 8 machines. The current RHEL 8 systems are
        \lstinline|gligar08| login node, \lstinline|doduo| cluster, and \lstinline|slaking| cluster.
        The created images can be used on any cluster.
    \item Due to the nature of \lstinline|--fakeroot| option, we recommend to write your singularity 
        image to a globally writable location, like \lstinline|/tmp|, or \lstinline|/local| directories. 
        Once the images is created, you should move it to your desired destination.
\end{itemize}

\subsection{Converting Docker images}

For more information on converting existing Docker images to Singularity images,
see \url{https://www.sylabs.io/guides/3.4/user-guide/singularity_and_docker.html}.

We strongly recommend the use of Docker Hub, see \url{https://hub.docker.com/} for more information.

\section{Execute our own script within our container}

Copy testing image from \lstinline|/apps/gent/tutorials/Singularity| to \lstinline|$VSC_SCRATCH|:

\begin{prompt}
%\shellcmd{cp /apps/gent/tutorials/Singularity/CentOS7\_EasyBuild.img \$VSC\_SCRATCH/}%
\end{prompt}

Create a job script like:

\begin{code}{bash}
#!/bin/sh

#PBS -o singularity.output
#PBS -e singularity.error
#PBS -l nodes=1:ppn=1
#PBS -l walltime=12:00:00


singularity exec $VSC_SCRATCH/CentOS7_EasyBuild.img ~/my_script.sh
\end{code}

Create an example \lstinline|myscript.sh|:

\begin{code}{bash}
#!/bin/bash

# prime factors
factor 1234567
\end{code}

\section{Tensorflow example}

We already have a Tensorflow example image, but you can also convert the Docker
image (see \url{https://hub.docker.com/r/tensorflow/tensorflow}) to a Singularity image yourself

Copy testing image from \lstinline|/apps/gent/tutorials| to \lstinline|$VSC_SCRATCH|:

\begin{prompt}
%\shellcmd{cp /apps/gent/tutorials/Singularity/Ubuntu14.04\_tensorflow.img \$VSC\_SCRATCH/}%
\end{prompt}

\begin{code}{bash}
#!/bin/sh
#
#
#PBS -o tensorflow.output
#PBS -e tensorflow.error
#PBS -l nodes=1:ppn=4
#PBS -l walltime=12:00:00
#

singularity exec $VSC_SCRATCH/Ubuntu14.04_tensorflow.img python ~/linear_regression.py
\end{code}

You can download \lstinline|linear_regression.py| from
\href{https://github.com/tensorflow/tensorflow/blob/r1.12/tensorflow/examples/get_started/regression/linear_regression.py}{the official Tensorflow repository}.

\section{MPI example}

It is also possible to execute MPI jobs within a container, but the following requirements apply:

\begin{itemize}
    \item Mellanox IB libraries must be available from the container (install the \lstinline|infiniband-diags|, \lstinline|libmlx5-1| and \lstinline|libmlx4-1| OS packages)
    \item Use modules within the container (install the \lstinline|environment-modules| or \lstinline|lmod| package in your container)
    \item Load the required module(s) before \lstinline|singularity| execution.
    \item Set \lstinline|C_INCLUDE_PATH| variable in your container if it is required during
        compilation time (\lstinline|export C_INCLUDE_PATH=/usr/include/x86_64-linux-gnu/:$C_INCLUDE_PATH| for Debian flavours)
\end{itemize}

Copy the testing image from \lstinline|/apps/gent/tutorials/Singularity| to \lstinline|$VSC_SCRATCH|

\begin{prompt}
%\shellcmd{cp /apps/gent/tutorials/Singularity/Debian8\_UGentMPI.img \$VSC\_SCRATCH/}%
\end{prompt}

For example to compile an
\href{https://github.com/open-mpi/ompi/blob/master/examples/ring_c.c}{MPI example}:

\begin{prompt}
%\shellcmd{module load intel}%
%\shellcmd{singularity shell \$VSC\_SCRATCH/Debian8\_UGentMPI.img}%
%\shellcmd{export LANG=C}%
%\shellcmd{export C\_INCLUDE\_PATH=/usr/include/x86\_64-linux-gnu/:\$C\_INCLUDE\_PATH}%
%\shellcmd{mpiicc ompi/examples/ring\_c.c -o ring\_debian}%
%\shellcmd{exit}%
\end{prompt}

Example MPI job script:

\begin{code}{bash}
#!/bin/sh

#PBS -N mpi
#PBS -o singularitympi.output
#PBS -e singularitympi.error
#PBS -l nodes=2:ppn=15
#PBS -l walltime=12:00:00

module load intel vsc-mympirun
mympirun --impi-fallback singularity exec $VSC_SCRATCH/Debian8_UGentMPI.img ~/ring_debian
\end{code}
