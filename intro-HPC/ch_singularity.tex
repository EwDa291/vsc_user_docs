\chapter{Singularity}
\label{ch:singularity}

\section{What is Singularity?}

Singularity is an open-source computer program that performs operating-system-level virtualization (also known as containerisation).

One of the main uses of Singularity is to bring containers and reproducibility to
scientific computing and the high-performance computing (HPC) world.
Using Singularity containers, developers can work in reproducible environments of their choosing and design,
and these complete environments can easily be copied and executed on other platforms.

For more general information about the use of Singularity, please see the official documentation at \url{https://www.sylabs.io/docs/}.

This documentation only covers aspects of using Singularity on the \hpcInfra infrastructure.

\section{Before using Singularity}

In order to use Singularity on the \hpcInfra infrastructure, you must be a member of the \lstinline|gsingularity| group.
See \autoref{subsec:joining-existing-group} for how join this group.

Please take into account that someone from the \hpcTeam will need to accept your request,
and it may take a couple of minutes for the accepted request gets trickled down into the
\hpcInfra systems.

\section{Restrictions on image location}

Some restrictions have been put in place on the use of Singularity. This is mainly done
for performance reasons and to avoid that the use of Singularity impacts other users on the system.

The Singularity image file must be located on either one of the scratch filesystems,
the local disk of the workernode you are using or \lstinline|/dev/shm|. The centrally provided \lstinline|singularity|
command will refuse to run using images that are located elsewhere, in particular on the \lstinline|$VSC_HOME|,
\lstinline|/apps| or \lstinline|$VSC_DATA| filesystems.

In addition, this implies that running containers images provided via a URL
(e.g., \lstinline|shub://...| or \lstinline|docker://...|) will not work.

If these limitations are a problem for you, please let us know via \hpcinfo.

\section{Available filesystems}

All HPC-UGent shared filesystems will be readily available in a Singularity container,
including the home, data and scratch filesystems, and they will be accessible via the
familiar \lstinline|$VSC_HOME|, \lstinline|$VSC_DATA*| and \lstinline|$VSC_SCRATCH*| environment variables.

\section{Singularity Images}

\subsection{Creating Singularity images}

Creating new Singularity images or converting Docker images requires admin privileges,
which is obviously not available on the \hpcInfra infrastructure.

As an alternative, you can either:

\begin{itemize}
    \item install Singularity on a local Linux system you have administrator privileges on,
        create you Singularity images there and upload them to the \hpcInfra infrastructure
    \item create Singularity images via Singularity Hub (\url{https://www.singularity-hub.org/}),
        and download them on the \hpcInfra infrastructure
\end{itemize}

We strongly recommend the use of Singularity Hub, see \url{https://singularity-hub.org/} for more information.

\subsection{Converting Docker images}

For more information on converting existing Docker images to Singularity images,
see \url{https://www.sylabs.io/guides/2.6/user-guide/singularity_and_docker.html}.
Note that this can also be done via Singularity Hub, see \url{https://singularity-hub.org/}.

\section{Execute our own script within our container}

Copy testing image from \lstinline|/apps/gent/tutorials/Singularity| to \lstinline|$VSC_SCRATCH|:

\begin{prompt}
%\shellcmd{cp /apps/gent/tutorials/Singularity/CentOS7\_EasyBuild.img \$VSC\_SCRATCH/singularity}%
\end{prompt}

Create a job script like:

\begin{code}{bash}
#!/bin/sh

#PBS -o singularity.output
#PBS -e singularity.error
#PBS -l nodes=1:ppn=1
#PBS -l walltime=12:00:00


irun $VSC_SCRATCH/singularity/CentOS7_EasyBuild.img ~/my_script.sh
\end{code}

Create an example \lstinline|myscript.sh|:

\begin{code}{bash}
#!/bin/bash

# prime factors
factor 1234567
\end{code}

\section{Tensorflow example}

We already have a Tensorflow example image, but you can also convert the Docker
image (see \url{https://hub.docker.com/r/tensorflow/tensorflow}) to a Singularity image yourself

Copy testing image from \lstinline|/apps/gent/tutorials| to \lstinline|$VSC_SCRATCH|:

\begin{prompt}
%\shellcmd{cp /apps/gent/tutorials/Singularity/Ubuntu14.04\_tensorflow.img \$VSC\_SCRATCH}%
\end{prompt}

\begin{code}{bash}
#!/bin/sh
#
#
#PBS -o tensorflow.output
#PBS -e tensorflow.error
#PBS -l nodes=1:ppn=4
#PBS -l walltime=12:00:00
#

irun $VSC_SCRATCH/Ubuntu14.04_tensorflow.img python ~/linear_regression.py
\end{code}

You can download \lstinline|linear_regression.py| from
\href{https://github.com/tensorflow/tensorflow/blob/master/tensorflow/examples/get_started/regression/linear_regression.py}{the official Tensorflow repository}.

\section{MPI example}

It is also possible to execute MPI jobs within a container, but the following requirements apply:

\begin{itemize}
    \item Mellanox IB libraries must be available from the container (install the \lstinline|infiniband-diags|, \lstinline|libmlx5-1| and \lstinline|libmlx4-1| OS packages)
    \item Use modules within the container (install the \lstinline|environment-modules| or \lstinline|lmod| package in your container)
    \item Load the required module(s) before \lstinline|irun| execution.
    \item Set \lstinline|C_INCLUDE_PATH| variable in your container if it is required during
        compilation time (\lstinline|export C_INCLUDE_PATH=/usr/include/x86_64-linux-gnu/:$C_INCLUDE_PATH| for Debian flavours)
\end{itemize}

Copy the testing image from \lstinline|/apps/gent/tutorials/Singularity| to \lstinline|$VSC_SCRATCH|

\begin{prompt}
%\shellcmd{cp /apps/gent/tutorials/Singularity/Debian8\_UGentMPI.img \$VSC\_SCRATCH/singularity}%
\end{prompt}

For example to compile an
\href{https://github.com/open-mpi/ompi/blob/master/examples/ring_c.c}{MPI example}:

\begin{prompt}
%\shellcmd{module load intel}%
%\shellcmd{irun \$VSC\_SCRATCH/singularity/Debian8\_UGentMPI.img}%
%\shellcmd{export LANG=C}%
%\shellcmd{export C\_INCLUDE\_PATH=/usr/include/x86\_64-linux-gnu/:\$C\_INCLUDE\_PATH}%
%\shellcmd{mpiicc ompi/examples/ring\_c.c -o ring\_debian}%
%\shellcmd{exit}%
\end{prompt}

Example MPI job script:

\begin{code}{bash}
#!/bin/sh

#PBS -N mpi
#PBS -o singularitympi.output
#PBS -e singularitympi.error
#PBS -l nodes=2:ppn=15
#PBS -l walltime=12:00:00

module load intel vsc-mympirun
mympirun --impi-fallback singularity exec $VSC_SCRATCH/singularity/Debian8_UGentMPI.img ~/ring_debian
\end{code}
