\chapter{Multi-core \& Multi-node Profiling}
\label{ch:ch04_multi_core_multi_node_profiling}

\renewcommand{\exampledir}{examples/ch04-parallel}

\begin{tip}
Find the source code for the examples in this Chapter in the directory:  ``\tilde/\exampledir''.
\end{tip}

PerfExpert will read hardware performance counter events of the various cores and/or nodes that participated in the parallel computing. PerfExpert will generate a summary report, which is based on the performance of all components.

As such, optimising a parallel application with PerfExpert is done in exactly the same way as a serial application. PerfExpert handles all additional complexity under the hood; the user gets the same kind of report.

We have created 3 parallel equivalents of the Matrix Multiplication program, one in OpenMP, one in MPI and one with a combination of both. It is interesting to see how much we can reduce the runtime of our application by spreading all the work over the different cores. We have adapted the size of the matrices in the examples to [2000x2000] as to increase the workload a bit.

First we, go to the directory of the examples of Chapter04:

\begin{prompt}
%\shellcmd{cd ~/\exampledir}%
\end{prompt}

\section{Multi-Core Profiling (with OpenMP)}
\label{sec:Multi_core_profilingi_OpenMP}

First we try the serial equivalent program, to have a base for the benchmark.

\begin{prompt}
%\shellcmd{gcc -g -o mm mm.c}%
%\shellcmd{time ./mm}%
real 1m17.981s
user 1m17.511s
sys 0m0.040s
\end{prompt}

Now, explore the OpenMP program, compile it and run.

\textbf{Note:} Compiling with OpenMP requires the additional OpenMP library to be linked in with the ``-fopenmp'' option.

\begin{prompt}
%\shellcmd{more mm\_omp.c}%
%\shellcmd{gcc -fopenmp --g -o mm\_omp mm\_omp.c}%
%\shellcmd{OMP\_NUM\_THREADS=8 perfexpert -c 0.2 ./mm\_omp}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 2.558715357 seconds (includes measurement overhead)
[perfexpert]    [2] 2.366958066 seconds (includes measurement overhead)
[perfexpert]    [3] 2.168382589 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
----------------------------------------------------------------------------
Total running time for mm_omp is 9.77 seconds between all 16 cores
The wall-clock time for mm_omp is approximately 0.61 seconds
Module mm_omp takes 99.81%\%% of the total runtime
Module libmonitor.so.0.0.0 takes 0.00%\%% of the total runtime
Module libgomp.so.1.0.0 takes 0.11%\%% of the total runtime
----------------------------------------------------------------------------
...
\end{prompt}

You notice that:
\begin{enumerate}
  \item  The runtime of the executable was sped up by a factor 3 to 4.
  \item  The ``mm\_omp'' program ran on all 16 cores.
  \item  The libgomp.so.1.0.0 library used 0.11\% of the execution time. This allows us to gather an idea of the ``overhead'' of OpenMP.
\end{enumerate}

\section{Multi-Node Profiling (with MPI)}
\label{sec:Multi_node_profiling_MPI}

The mm\_mpi.c program contains the Matrix Multiplication program developed with MPI. First we compile and run the program. Do not forget to request a private node for this task.

For VSC, first load the MPI module:

\begin{prompt}
%\shellcmd{module load impi}%
\end{prompt}

\begin{tabular}{|p{0.3\textwidth}|p{0.7\textwidth}} \hline
\textbf{Institute}          & \textbf{Command} \\ \hline
TACC                        & \$ \textbf{module load impi} \\ \hline
VSC                         & \$ \textbf{module load impi} \\ \hline
VUBrussel                   & \$ \textbf{module load openmpi/1.6.5/gcc/4.8.2}
                              \$ \textbf{module load impi} \\ \hline
\end{tabular}


\begin{prompt}
%\shellcmd{mpicc -o mm\_mpi mm\_mpi.c}%
%\shellcmd{\mpirun mm\_mpi}%
\end{prompt}

The command line to run PerfExpert includes the MPI launcher script (i.e., mpirun). If we would start ``PerfExpert 0.1 mpirun mm\_mpi'', we would analyse the performance of the MPI launcher instead of the performance of your application. This is of course not what we want. You can use the ``\textit{-p}'' (prefix) command line argument of PerfExpert to define a prefix to the normal command line. In our case, we want to set the MPI launcher and all its arguments (e.g., -p
``mpirun'').

The command becomes:

\begin{prompt}
%\shellcmd{perfexpert -p ``mpirun'' -c 0.1 ./mm\_mpi}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 11.375589158 seconds (includes measurement overhead)
[perfexpert]    [2] 11.637017437 seconds (includes measurement overhead)
[perfexpert]    [3] 10.165687700 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
----------------------------------------------------------------------------
Total running time for mm_mpi is 126.54 seconds between all 16 cores
The wall-clock time for mm_mpi is approximately 7.91 seconds
...
\end{prompt}

No difference, whatsoever, in the report that is displayed to the user.

\section{Profiling with OpenMP and MPI}
\label{sec:Profiling_with_OpenMP_MPI}

The ``\textit{mm\_mpi\_omp.c}'' program contains the Matrix Multiplication program developed with OpenMP and MPI. The trick with OpenMP and MPI is not any different.

First we compile and run the program.

\begin{prompt}
%\shellcmd{mpicc -fopenmp -o mm\_mpi\_omp mm\_mpi\_omp.c}%
The command becomes:
%\shellcmd{perfexpert -p "mpirun" 0.1  ./mm\_mpi\_omp}%
[perfexpert] Collecting measurements [hpctoolkit]
[perfexpert]    [1] 11.759415785 seconds (includes measurement overhead)
[perfexpert]    [2] 14.620955584 seconds (includes measurement overhead)
[perfexpert]    [3] 11.391528953 seconds (includes measurement overhead)
[perfexpert] Analysing measurements
...
\end{prompt}

