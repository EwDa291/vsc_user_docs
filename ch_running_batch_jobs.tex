\chapter{Running batch jobs}

In order to have access to the compute nodes of a cluster, one has to make use of the job system. The system software that handles your batch jobs consists of two pieces: the queue- and resource manager \textbf{TORQUE} and the scheduler \textbf{Moab}. Together, TORQUE and Moab provide a suite of commands for submitting jobs, altering some of the properties of waiting jobs (such as reordering or deleting them), monitoring their progress and killing ones that are having problems or are no longer needed. Only the most commonly used commands are mentioned here.

When you connect to the UA-HPC, you have access to (one of) the \textbf{login nodes} of the cluster. There you can prepare the work you want to get done on the cluster by, e.g., installing or compiling programs, setting up data sets, etc. The computations however, should not be performed on this login node. The actual work is done on the cluster's \textbf{compute nodes}. These compute nodes are managed by the job scheduling software (MOAB) and a Resource Manager (TORQUE), which decides when and on which compute nodes the jobs can run. It is usually not necessary (and in some cases not permitted) to log on to the compute nodes directly. A user can (and should) monitor his or her jobs periodically as they run, but does not have to remain logged in the entire time.

\includegraphics*[width=5.78in, height=3.94in, keepaspectratio=false]{img0400}

The documentation in this ``Running batch jobs'' section includes a description of the general features of job scripts, how to submit them for execution and how to monitor their progress.

\section{Defining and submitting your job}

Usually, you will want to have your program running in batch mode, as opposed to interactively as you may be accustomed to. The point is that the program must be able to start and run without user intervention, i.e., without you having to enter any information or to press any buttons during program execution. All the necessary input or required options have to be specified on the command line, or needs to be put in input or configuration files.

As an example, we will run a MATLAB script, which you will find in the examples subdirectory on the UA-HPC. When you received an account to the UA-HPC, a subdirectory with examples was automatically generated for you.

Remember that you have copied the contents of the HPC examples directory to your home directory, so that you have your \textbf{own personal }copy (editable and over-writable) and that you can start using the examples. If you haven't done so:
\begin{prompt}
$ %\textbf{cd}%
$ %\textbf{cp --r ./tutorials/calcua/examples \~/}%
\end{prompt}

First go to the directory with the first examples by entering the command:
\begin{prompt}
$ %\textbf{cd \~/examples/Chapter04\_Batch}%
\end{prompt}

Each time you want to execute a program on the UA-HPC, you'll need 2 things:

\begin{enumerate}
\item  \textbf{The executable:} The program to execute from the end-user, together with its peripheral input files, databases and/or command options.
\item  \textbf{A configuration script }(also called a job-script), which will define the computer resource requirements of the program, the required additional software packages and which will start the actual executable.  The UA-HPC needs to know:

\begin{enumerate}
\item  the type of compute nodes;
\item  the number of CPU's;
\item  the amount of memory;
\item  the expected duration of the execution time;
\item  the name of the files which will contain the output (i.e. stdout) and error (i.e. stderr) messages;
\item  what executable to start, and its arguments.
\end{enumerate}
\end{enumerate}

Later on, the UA-HPC-user shall have to define (or to adapt) their own configuration scripts. For now, all required configuration scripts for the exercises are provided for you in the examples subdirectories.

List and check the contents with:
\begin{prompt}
$ %\textbf{ls -l}%
total 512
-rw-r--r-- 1 vsc20167 193 Sep 11 10:34 fibo.pbs
-rw-r--r-- 1 vsc20167 609 Sep 11 10:25 fibo.pl
\end{prompt}

In this directory you find a PERL script (named `fibo.pl') and a PBS configuration script (named `fibo.pbs').

\begin{enumerate}
\item  The PERL script calculates the first 30 Fibonacci numbers.
\item  The job-script is actually a standard Unix/Linux shell script that contains a few extra comments at the beginning that specify directives to PBS.  These comments all begin with \textbf{\#PBS}.
\end{enumerate}

We will first execute the program locally (i.e., on your current login-node), so that you can see what the program does.

On the command line, you would run this using:
\begin{prompt}
$ %\textbf{./fibo.pl}%
[0] -$>$ 0
[1] -$>$ 1
[2] -$>$ 1
[3] -$>$ 2
[4] -$>$ 3
[5] -$>$ 5
[6] -$>$ 8
[7] -$>$ 13
[8] -$>$ 21
[9] -$>$ 34
[10] -$>$ 55
[11] -$>$ 89
[12] -$>$ 144
[13] -$>$ 233
[14] -$>$ 377
[15] -$>$ 610
[16] -$>$ 987
[17] -$>$ 1597
[18] -$>$ 2584
[19] -$>$ 4181
[20] -$>$ 6765
[21] -$>$ 10946
[22] -$>$ 17711
[23] -$>$ 28657
[24] -$>$ 46368
[25] -$>$ 75025
[26] -$>$ 121393
[27] -$>$ 196418
[28] -$>$ 317811
[29] -$>$ 514229
\end{prompt}

\underbar{Remark}: Recall that you have now executed the PERL script locally on one of the login-nodes of the UA-HPC cluster.  Of course, this is not our final intention; we want to run the script on any of the compute nodes. Also, it is not considered as good practice, if you ``abuse'' the login-nodes for testing your scripts and executable's. It will be explained later on how you can reserve your own compute-node by (by opening an interactive session) to test your software. But for the sake of acquiring a good understanding of what is happening, you are pardoned for this example.

The job-script contains a description of the job by specifying the command that need to be executed on the compute node:
\begin{prompt}
$ %\textbf{cat fibo.pbs}%
\#!/bin/bash -l
cd \$PBS\_O\_WORKDIR
./fibo.pl
\end{prompt}

So, jobs are submitted as scripts (bash, perl, python, etc.), which specify the parameters related to the jobs such as expected runtime (walltime), mail notification, etc. These parameters can also be specified on the command line.

This job script that can now be submitted to the cluster's job system for execution, using the qsub (Queue SUBmit) command:
\begin{prompt}
$ %\textbf{qsub fibo.pbs}%
433253.master1.turing.antwerpen.vsc
\end{prompt}

The qsub command returns a job identifier on the HPC cluster. The important part is the number (e.g., `433253'); this is a unique identifier for the job and can be used to monitor and manage your job.

Go and drink some coffee\dots . but not too long.

When you return, check the contents of the directory:
\begin{prompt}
$ %\textbf{ls -l}%
total 768
-rw-r--r-- 1 vsc20167 vsc20167   44 Feb 28 13:33 fibo.pbs
-rw------- 1 vsc20167 vsc20167    0 Feb 28 13:33 fibo.pbs.e521167
-rw------- 1 vsc20167 vsc20167 1010 Feb 28 13:33 fibo.pbs.o521167
-rwxrwxr-x 1 vsc20167 vsc20167  302 Feb 28 13:32 fibo.pl*
\end{prompt}

Explore the contents of the 2 new files:
\begin{prompt}
$ %\textbf{more fibo.pbs.o521167}%
$ %\textbf{more fibo.pbs.e521167}%
\end{prompt}

These files are used to store the standard output and error that would otherwise be shown in the terminal window. By default, they have the same name as that of the PBS script, i.e. 'fibo.pbs' as base name, followed by the extension '.o' (output) and '.e' (error), respectively, and the job number ('433253' for this example). The error file will be empty, at least if all went well. If not, it may contain valuable information to determine and remedy the problem that prevented a successful run. The standard output file will contain the results of your calculation (here, the output of the matlab script)

\section{Monitoring and managing your job(s)}

Using the job ID that \textit{qsub} returned, there are various ways to monitor the status of you job, e.g.,

To get the status information on your job:
\begin{prompt}
$ %\textbf{qstat $<$jobid$>$}%
\end{prompt}

To show an estimated start time for you job (note that this may be very inaccurate):
\begin{prompt}
$ %\textbf{showstart $<$jobid$>$}%
\end{prompt}

To show the status, but also the resources required by the job, with error messages that may prevent your job from starting:
\begin{prompt}
$ %\textbf{checkjob $<$jobid$>$}%
\end{prompt}

To show on which compute nodes you job is running, at least, when it is running:
\begin{prompt}
$ %\textbf{qstat -n $<$jobid$>$}%
\end{prompt}

To remove a job from the queue so that it will not run, or to stop a job that is already running.
\begin{prompt}
$ %\textbf{qdel $<$jobid$>$}%
\end{prompt}

When you have submitted several jobs (or you just forgot about the job ID), you can retrieve the status of all your jobs that are submitted and are not yet finished using (uid is your VSC user name on the system):
\begin{prompt}
$ %\textbf{qstat -u $<$uid$>$}%
master1.turing.antwerpen.vsc:
                          Req'd     Req'd   Elap
Job ID      Username Queue   Jobname      SessID NDS   TSK  Memory Time S Time
-------------------- -------- -------- ---------------- ------ ----- --- ------ ----- - ------------------
433295.master1.t  vsc20167 qreg  fibo.pbs  --        1         1        --       01:00 R  13:41
433296.master1.t  vsc20167 qreg  fibo.pbs  --        1         1        --       01:00 Q  00:00
\end{prompt}

Here:
\begin{enumerate}
\item  \textbf{Job id}: the job's unique identifier
\item  \textbf{Username}: the user that owns the job
\item  \textbf{Queue}: the queue the job is in
\item  \textbf{Jobname}: the name of the job
\item  \textbf{NDS}: the number of chunks or nodes requested by the job
\item  \textbf{TSK}: the number of CPUs requested by the job
\item  \textbf{Req'd Time}: the requested walltime for the job here:
\item  \textbf{Elap Time}: the elapsed walltime for the job here:
\end{enumerate}

The state S can be any of  the following:
\begin{tabular}{|p{0.4in}|p{3.6in}|} \hline
\textbf{State} & \textbf{Meaning} \\ \hline
\textbf{Q} & The job is \textbf{queued} and is waiting to start. \\ \hline
\textbf{R} & The job is currently \textbf{running}. \\ \hline
\textbf{E} & The job is currently \textbf{exiting} after having run. \\ \hline
\textbf{C} & The job is \textbf{completed} after having run. \\ \hline
\textbf{H} & The job has a user or system \textbf{hold} on it and will not be eligible to run until the hold is removed. \\ \hline
\end{tabular}

\section{Examining the queue}

As we learned above, MOAB is the software application that actually decides when to run your job and what resources your job will run on. You can look at the queue by using either the PBS \textit{qstat} command or the MOAB \textit{showq} command. By default, q\textit{stat} will display the queue ordered by \textit{JobID}, whereas \textit{showq} will display jobs grouped by their state ("running," "idle," or "hold") then ordered by priority.  Therefore, \textit{showq }is often more useful.

The \textit{showq} command displays information about active ("running"), eligible ("idle"), blocked ("hold"), and/or recently completed jobs. To get a summary:
\begin{prompt}
$ %\textbf{showq -s}%
active jobs: 163
eligible jobs: 133
blocked jobs: 243
Total jobs:  539
\end{prompt}

And to get the full detail of all the jobs, which are in the system:
\begin{prompt}
$ %\textbf{showq}%
active jobs------------------------
JOBID      USERNAME   STATE PROCS REMAINING          STARTTIME
428024    vsc20117  Running   8   2:57:32  Mon Sep  2 14:55:05
442526    vsc20066  Running  24   7:13:34  Sun Sep 22 19:11:07
442527    vsc20066  Running  24  13:03:08  Mon Sep 23 01:00:41
\dots
153 active jobs 1307 of 2472 processors in use by local jobs (52.87\%)
153 of 167 nodes active      (91.62\%)

eligible jobs----------------------
JOBID     USERNAME  STATE PROCS   WCLIMIT            QUEUETIME
442604    vsc20030   Idle  48  7:00:00:00  Sun Sep 22 16:39:13
442605    vsc20030   Idle  48  7:00:00:00  Sun Sep 22 16:46:22
442725    vsc20133   Idle   8  1:00:00:00  Sun Sep 22 22:15:31
\dots

135 eligible jobs

blocked jobs-----------------------
JOBID   USERNAME     STATE PROCS WCLIMIT            QUEUETIME
441237  vsc20034      Idle   8 3:00:00:00 Thu Sep 19 15:53:10
441238  vsc20034      Idle   8 3:00:00:00 Thu Sep 19 15:53:10
442536  vsc20020  UserHold  40 3:00:00:00 Sun Sep 22 00:14:22
\dots
252 blocked jobs
Total jobs:  540
\end{prompt}

There are 3 categories, the \textbf{active}, \textbf{eligible} and \textbf{blocked} jobs.

\begin{enumerate}
\item  \textbf{Active jobs} are jobs that are running or starting and that consume computer resources. The amount of time remaining (w.r.t. walltime, sorted to earliest completion time) and the start time ?are displayed. This will give you an idea about the foreseen completion time. These jobs could be in a number of states:

\begin{enumerate}
\item  \textbf{Started} : attempting to start, performing pre-start tasks
\item  \textbf{Running} : currently executing the user application
\item  \textbf{Suspended} : has been suspended by scheduler or admin ?(still in place on the allocated resources, not executing)
\item  \textbf{Cancelling}: has been cancelled, in process of cleaning up
\end{enumerate}
\item  \textbf{Eligible jobs} are jobs that are waiting in the queues and are considered eligible ?for both scheduling and backfilling.  They are all in the idle job state and do not violate any fairness policies or do not have any job holds in place. The requested walltime is displayed, and the list is ordered by job priority.
\item  \textbf{Blocked jobs} are jobs that are ineligible to be run or queued.  These jobs could be in a number of states for the following reasons:

\begin{enumerate}
\item  \textbf{Idle} when the job violates a fairness policy
\item  \textbf{Userhold} or systemhold when it is user or administrative hold
\item  \textbf{Batchhold} when the requested resources are not available or the resource manager has repeatedly failed to start the job
\item  \textbf{Deferred} when a temporary hold when the job has been unable to start after a specified number of attempts
\item  \textbf{Notqueued} when scheduling daemon is unavailable
\end{enumerate}
\end{enumerate}

\section{Specifying job requirements}

Without giving more information about your job upon submitting it with \textit{qsub}, default values will be assumed that are almost never appropriate for real jobs.

It is important to estimate the resources you need to successfully run your program, such as the amount of time the job will require, the amount of memory it needs, the number of CPUs it will run on, etc. This may take some work, but it is necessary to ensure your jobs will run properly.

\subsection{Generic resource requirements}

The qsub command takes several options to specify the requirements, of which we list the most commonly used once below.
\begin{prompt}
-l walltime=2:30:00
\end{prompt}

For the simplest cases, only the amount of maximum estimated execution time (called "walltime") is really important. Here, the job will not require more than 2 hours, 30 minutes to complete. As soon as the job would take more time, it will be `killed' (terminated) by the job scheduler.  As such, it does not harm if you \textit{slightly} overestimate the maximum execution time.
\begin{prompt}
-l mem=4gb
\end{prompt}

The job requires no more than 4 Gb of memory.

\begin{prompt}
-l nodes=5:ppn=2
\end{prompt}
The job requires 5 compute nodes with two cores on each node (ppn stands for "processors per node", where processor is used to refer to indivual cores)

\begin{prompt}
-l nodes=1:westmere
\end{prompt}
The job requires just one node, but it should have an Intel Westmere processor. A list with site-specific properties can be found in the next section.

These options can either be specified on the command line, e.g.
\begin{prompt}
$ %\textbf{qsub -l nodes=1:harpertown,mem=2gb fibo.pbs}%
\end{prompt}

or in the job-script itself using the \#PBS-directive, so 'fibo.pbs' could be modified to:
\begin{prompt}
\#!/bin/bash -l
\#PBS -l nodes=1:westmere
\#PBS -l mem=2gb
cd \$PBS\_O\_WORKDIR
./fibo.pl
\end{prompt}

Note that the resources requested on the command line will override those specified in the PBS file.

\subsection{Available job categories (Torque queues)}

In order to guarantee a fair share access to the computer resources to all users, only a limited number of jobs with certain walltimes are possible per user.

We therefore classify the submitted jobs in categories (confusingly also called queues), depending on the their walltime specification.  A user is allowed to run up to a certain maximum number of jobs in each of these walltime categories.

The currently defined queue categories (with walltime limits) for the UA-HPC are:

\begin{tabular}{|p{0.6in}|p{0.7in}|p{0.8in}|p{0.7in}|p{0.7in}|} \hline
\textbf{Queue\newline category} & \multicolumn{2}{|p{1.5in}|}{\textbf{Walltime}} & \multicolumn{2}{|p{1.4in}|}{\textbf{Max \# Jobs}} \\ \hline
\textbf{} & \textbf{Minimum\newline / from\newline (value not included)} & \textbf{Maximum \newline / to \newline (value included)} & \textbf{Queuable} & \textbf{Runnable} \\ \hline
qshort & 0 & 1 hour & 1000 & 500 \\ \hline
qreg & 1 hour & 1 day & 1000 & 100 \\ \hline
qlong & 1 day & 3 days & 500 & 100 \\ \hline
qxlong & 3 days & 7 days & 50 & 25 \\ \hline
qxxlong & 7 days & 21 days & 8 & 2 \\ \hline
\end{tabular}

\underbar{Remark:} As the infrastructure of the UA-HPC is constantly expanding and evolving, it can be anticipated that also the limits of the categories can be changed over time.

When a user submits a job with a walltime of 15 days, the queue manager will put the job in the qxxlong category.  The user can submit up to 8 jobs with this high walltimes (queable = 8), but only 2 of those jobs will be eligible for execution (runnable=2) at the same time.  A detailed description of the fair-share mechanisms will follow in Chapter 9. For longer running jobs, \textit{checkpointing} (See Chapter 13) is necessary.

Apart from specifying the \textit{walltime}, you can also explicitly define the queue you're submitting your job to.

To specify the queue, add:
\begin{prompt}
-q queuename
\end{prompt}
to the qsub command line, or
\begin{prompt}
\#PBS -q queuename
\end{prompt}

to the job-script, where \textit{queuename} is one of the possible queues shown above.

A maximum \textit{walltime} is associated with each queue.

The queue category logic is:

\begin{tabular}{|p{0.9in}|p{1.3in}|p{1.7in}|} \hline
\textbf{} & \textbf{No walltime specified} & \textbf{Walltime specified} \\ \hline
\textbf{No queue \newline specified} & The job is submitted in the qshort queue with a walltime of 1 hour. & The job is submitted in the proper queue in accordance with the given walltime. \\ \hline
\textbf{Queue \newline specified} & The job is submitted in your specified queue with q walltime of 1 hour.  & The job is submitted in the specified queue with the specified walltime. If the specified \textit{walltime} is larger than the maximal \textit{walltime} of the requested queue, the job cannot be submitted. \\ \hline
\end{tabular}

\underbar{Remark:} It is highly recommended to specify a walltime at all times in all your job scripts. Only for some short test-runs, a walltime specification could be omitted.

When a user tries to submit more jobs in a certain walltime category than the maximum number of queable jobs, the submission will fail. The scheduler will also enforce that no more than the maximum of runnable jobs per category are being executed at the same time. \underbar{}

To get basic information about the queues, the command "qstat -q" is used:
\begin{prompt}
$ %\textbf{qstat -q}%
server: master1.turing.antwerpen.vsc

Queue      Memory CPU Time Walltime Node  Run Que Lm  State
----------------- -------- -------- ----  --- --- --  -----
qdef         --      --       --      --    0   0 --   E R
qlong        --      --    72:00:00   --  113 117 --   E R
qreg         --      --    24:00:00   --    7  16 --   E R
qshort       --      --    01:00:00   --    1   0 --   E R
qxlong       --      --    168:00:0   --   23   0 --   E R
qxxlong      --      --    504:00:0   --    5   0 --   E R
                                         ----- -----
                                          149  133
\end{prompt}

The number of jobs currently running in the queue is shown in the Run column, whereas the number of jobs waiting to get started is shown in the Queue column.  The maximum walltime that is accepted by the queue is shown in the Walltime column.

To obtain more detailed information on the queues (e.g., qxlong) the following command can be used:
\begin{prompt}
$ %\textbf{qstat -f -Q qxlong}%
\end{prompt}

This will list additional restrictions such as the maximum walltime of the jobs and the maximum number of jobs that a user can have in that queue.

\subsection{Node-specific properties}

The following table contains some node-specific properties that can be used to make sure the job will run on nodes with a specific CPU or interconnect. Note that these properties may very over the different VSC sites.

\begin{tabular}{|p{0.7in}|p{3.3in}|} \hline
\textbf{property} & \textbf{explanation} \\ \hline
harpertown & only use Intel processors from the Harpertown family (54xx) \\ \hline
westmere & only use Intel processors from the Westmere family (56xx) \\ \hline
ib & use Infiniband interconnect  \\ \hline
\end{tabular}

To get a list of all properties defined for all nodes, enter
\begin{prompt}
$ %\textbf{pbsnodes}%
\end{prompt}

This list will also contain properties referring to, e.g., network components, rack number, etc.

\section{Job output and error files}

At some point your job finishes, so you may no longer see the job ID in the list of jobs when you run \textit{qstat (since it will only be listed for a few minutes after completion with state "C")}. After your job finishes, you should see the standard output and error of your job in two files, located by default in the directory where you issued the \textit{qsub} command.


When you navigate to that directory and list its contents, you should see them:
\begin{prompt}
$ %\textbf{ls -l }%
total 1024
-rw-r--r-- 1 vsc20167  609 Sep 11 10:54 fibo.m
-rw-r--r-- 1 vsc20167   68 Sep 11 10:53 fibo.pbs
-rw------- 1 vsc20167   52 Sep 11 11:03 fibo.pbs.e433253
-rw------- 1 vsc20167 1307 Sep 11 11:03 fibo.pbs.o433253
\end{prompt}

In our case, our job has created both output (`fibo.pbs.\textbf{o}433253') and error files (`fibo.pbs.\textbf{e}433253') containing info written to \textit{stdout} and \textit{stderr} respectively.

Inspect the generated output and error files:
\begin{prompt}
$ %\textbf{cat fibo.pbs.o433253}%
\dots
$ %\textbf{cat fibo.pbs.e433253}%
\dots
\end{prompt}

\section{E-mail notifications}

\subsection{Upon job failure}

Whenever a job fails, an e-mail will be sent to the e-mail address that's connected to your $<$vsc-account$>$. This is the e-mail address that is linked to the university account, which was used during the registration process.

You can force a job to fail by specifying an unrealistic wall-time for the previous example.  Lets give the `\textit{fibo.pbs}' job just one second to complete:
\begin{prompt}
$ %\textbf{qsub -l walltime=0:00:01 fibo.pbs}%
\end{prompt}

Now, lets hope that the UA-HPC did not manage to run the job within one second, and you will get an e-mail, informing you that:

\begin{prog}
PBS Job Id: 442805.master1.turing.antwerpen.vsc
Job Name:   \textbf{fibo.pbs}
Exec host:  r2e2cn05.turing.antwerpen.vsc/0
Aborted by PBS Server
Job exceeded some resource limit (walltime, mem, etc.). Job was aborted.
See Administrator for help
\end{prog}

\subsection{Generate your own e-mail notifications}

You can instruct the UA-HPC to send an e-mail to your e-mail address whenever a job \textbf{b}egins, \textbf{e}nds and/or \textbf{a}borts, by adding the following lines to the job-script 'fibo.pbs':
\begin{prompt}
\#PBS -m b?
\#PBS -m e?
\#PBS -m a\#PBS -M $<$your e-mail address$>$
\end{prompt}
or
\begin{prompt}
\#PBS -m abe
\#PBS -M  $<$your e-mail address$>$
\end{prompt}

These options can also be specified on the command line.
Try it and see what happens:
\begin{prompt}
$ %\textbf{qsub -mabe -M $<$your e-mail address$>$ fibo.pbs}%
\end{prompt}

You don't have to specify the e-mail address. In such cases, the system will use the e-mail address, which is connected to your VSC account.

\#IF Chapter05\_Mac
