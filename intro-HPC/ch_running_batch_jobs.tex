\chapter{Running batch jobs}
\label{ch:running-batch-jobs}

In order to have access to the compute nodes of a cluster, you have to use
the job system. The system software that handles your batch jobs consists of
two pieces: the queue- and resource manager \strong{TORQUE} and the scheduler
\strong{Moab}. Together, TORQUE and Moab provide a suite of commands for
submitting jobs, altering some of the properties of waiting jobs (such as
reordering or deleting them), monitoring their progress and killing ones that
are having problems or are no longer needed. Only the most commonly used
commands are mentioned here.

\begin{center}
\includegraphics*[width=5.78in, height=3.94in, keepaspectratio=false]{ch4-pbs-overview}
\end{center}

When you connect to the \hpc, you have access to (one of) the \strong{login
nodes} of the cluster. There you can prepare the work you want to get done on
the cluster by, e.g., installing or compiling programs, setting up data sets,
etc. The computations however, should not be performed on this login node. The
actual work is done on the cluster's \strong{compute nodes}. Each compute node
contains a number of CPU \strong{cores}. The compute nodes are managed by the
job scheduling software (Moab) and a Resource Manager (TORQUE), which decides
when and on which compute nodes the jobs can run. It is usually not necessary
%% TODO: where?
%(and in some cases not permitted)
to log on to the
compute nodes directly
\ifbrussel
and is only allowed on the nodes where you have a job running
\fi
\ifgent
and is only allowed on the nodes where you have a job running
\fi
. Users can (and should) monitor their jobs
periodically as they run, but do not have to remain connected to the \hpc the entire
time.

The documentation in this ``Running batch jobs'' section includes a description
of the general features of job scripts, how to submit them for execution and
how to monitor their progress.

\section{Modules}
\label{sec:modules}

Software installation and maintenance on a \hpc cluster such as the VSC
clusters poses a number of challenges not encountered on a workstation or a
departmental cluster. We therefore need a system on the \hpc, which is able
to easily activate or deactivate the software packages that you require for
your program execution.

\subsection{Environment Variables}

The program environment on the \hpc is controlled by pre-defined settings,
which are stored in environment (or shell) variables. For more information about environment variables, see
\href{\LinuxManualURL#sec:environment-variables}{the chapter ``Getting started'', section ``Variables'' in the intro to Linux}.

All the software packages that are installed on the \hpc cluster require
different settings. These packages include compilers, interpreters,
mathematical software such as MATLAB and SAS, as well as other applications and
libraries.

\subsection{The module command}

In order to administer the active software and their environment variables,
the module system has been developed, which:

\begin{enumerate}
\item  Activates or deactivates \emph{software packages} and their dependencies.
\item  Allows setting and unsetting of \emph{environment variables}, including
    adding and deleting entries from database-type environment variables.
\item  Does this in a \emph{shell-independent} fashion (necessary information
    is stored in the accompanying module file).
\item  Takes care of \emph{versioning aspects}: For many libraries, multiple
    versions are installed and maintained. The module system also takes care of
    the versioning of software packages. For instance, it does not allow multiple
    versions to be loaded at same time.
\item  Takes care of \emph{dependencies}: Another issue arises when one considers
    library versions and the dependencies they require. Some software requires an
    older version of a particular library to run correctly (or at all). Hence a
    variety of version numbers is available for important libraries. Modules
    typically load the required dependencies automatically.
\end{enumerate}

This is all managed with the \lstinline|module| command, which is explained in the next sections.

\ifusinglmod
There is also a shorter \lstinline|ml| command that does exactly the same as the \lstinline|module|
command and is easier to type. Whenever you see a \lstinline|module| command, you can replace
\lstinline|module| with \lstinline|ml|
\fi


\subsection{Available modules}

A large number of software packages are installed on the \hpc clusters. A
list of all currently available software can be obtained by typing:

\begin{prompt}
%\shellcmd{module available}%
\end{prompt}

It's also possible to execute \lstinline|module av| or \lstinline|module avail|,
these are shorter to type and will do the same thing.

This will give some output such as:

\inputsite{available-modules}

This gives a full list of software packages that can be loaded.

\strong{The casing of module names is important}: lowercase and uppercase letters
matter in module names.

\subsection{Organisation of modules in toolchains}

The amount of modules on the VSC systems can be overwhelming, and it is not
always immediately clear which modules can be loaded safely together if you
need to combine multiple programs in a single job to get your work done.

Therefore the VSC has defined so-called \strong{toolchains}.
A toolchain contains a C/C++ and Fortran compiler, a MPI library and some basic
math libraries for (dense matrix) linear algebra and FFT. Two toolchains are
defined on most VSC systems. One, the \lstinline|intel| toolchain, consists of the Intel
compilers, MPI library and math libraries. The other one, the \lstinline|foss| toolchain,
consists of Open Source components: the GNU compilers, OpenMPI, OpenBLAS and
the standard LAPACK and ScaLAPACK libraries for the linear algebra operations
and the FFTW library for FFT. The toolchains are refreshed twice a year,
which is reflected in their name.

E.g., \texttt{foss-\the\year} is the first version of the \lstinline|foss| toolchain in \the\year.

The toolchains are then used to compile a lot of the software installed on
the VSC clusters. You can recognise those packages easily as they all contain the
name of the toolchain after the version number in their name. Packages
compiled with the same toolchain and toolchain version will typically work
together well without conflicts.

For some clusters, additional toolchains are defined, e.g., to take advantage
of specific properties of that cluster such as GPU accelerators or special
interconnect features that require a vendor-specific MPI-implementation.

\subsection{Activating and de-activating modules}
\label{subsec:activating-and-deactivating-modules}

To ``activate'' a software package, you load the corresponding module file
using the \lstinline|module load| command:

\begin{prompt}
%\shellcmd{module load \emph{example}}%
\end{prompt}

This will load the most recent version of \emph{example}.

For some packages, multiple versions are installed; the load
command will automatically choose the default version (if it was set by the
system administrators) or the most recent version otherwise (i.e., the
lexicographical last after the \lstinline|/|).

\strong{However, the user can (and probably should, to avoid
surprises when newer versions are installed) specify a particular version},
e.g.,

\begin{prompt}
%\shellcmd{module load \emph{secondexample/2.7-intel-2018a}}%
\end{prompt}

Obviously, you need to keep track of the modules that are currently
loaded. If you executed the two load commands stated above, you will get the
following:

\begin{prompt}
%\shellcmd{module list}%
Currently Loaded Modulefiles:
  1) example/1.2.3-intel-2018a        5) intel/2018a
  2) secondexample/2.7-intel-2018a          6) examplelib/1.2-intel-2018b
  3) GCCcore/4.9.3                          7) binutils/2.25-GCCcore-4.9.3
  4) ifort/2018.1.150-GCC-4.9.3-2.25        8) %\ldots{}%
\end{prompt}


It is important to note at this point that other modules (e.g., \lstinline|intel/2018a|)
are also listed, although the user did not explicitly load them. This is
because \lstinline|secondexample/2.7-intel-2018a| depends on it (as indicated in its name),
and the system administrator specified that the \lstinline|intel/2014a| module should
be loaded whenever the \lstinline|secondexample| module is loaded. There are advantages and
disadvantages to this, so be aware of automatically loaded modules whenever
things go wrong: they may have something to do with it!

In fact, an easy way to check the components and version numbers of those
components of a toolchain is to simply load the toolchain and then list the
modules that are loaded.

Modules need not be loaded one by one; the two \lstinline|module load| commands can be
combined as follows:

\begin{prompt}
%\shellcmd{module load \emph{example/1.2.3-intel-2018a} \emph{secondexample/2.7-intel-2018a}}%
\end{prompt}

This will load the two modules as well as their dependencies.

To unload a module, one can use the \lstinline|module unload| command. It works
consistently with the load command, and reverses the latter's effect. However,
the dependencies of the package are NOT automatically unloaded; you will have to
unload the packages one by one. When the \lstinline|secondexample| module is unloaded,
only the following modules remain:

\begin{prompt}
%\shellcmd{module unload \emph{secondexample}}%
%\shellcmd{module list}%
Currently Loaded Modulefiles:
    1) example/1.2.3-intel-2018a        5) intel/2018a
    3) GCCcore/4.9.3                          6) binutils/2.25-GCCcore-4.9.3
    4) ifort/2018.1.150-GCC-4.9.3-2.25        7) examplelib/1.2-intel-2018b
\end{prompt}

\ifusinglmod
To unload the \lstinline|secondexample| module, you can also use \lstinline|ml -secondexample|.
You can also just use the \lstinline|ml| command without arguments to list loaded modules.
\fi

Notice that the version was not specified: there can only be one version of a module
loaded at a time. However, checking the list of currently loaded modules is always a good idea.

\subsection{Purging all modules}
\label{subsec:purging-modules}

In order to unload all modules at once, and hence be sure to start in a clean
state, you can use:

\begin{prompt}
%\shellcmd{module purge}%
\end{prompt}

\ifgent
This is always safe: the \lstinline|cluster| module (the module that specifies what cluster
jobs will get submitted to) will not be unloaded (because it's a ``sticky'' module).
\else
However, on some VSC clusters you may be left with a very empty list of available
modules after executing \lstinline|module purge|. On those systems, \lstinline|module av| will show you a list of
modules containing the name of a cluster or a particular feature of a section of
the cluster, and loading the appropriate module will restore the module list
applicable to that particular system.
\fi

\subsection{Explicit version numbers}
\label{subsec:explicit-version-numbers}

As a rule, once a module has been installed on the cluster, the executables or
libraries it comprises are never modified. This policy ensures that the user's
programs will run consistently, at least if the user specifies a specific
version. \strong{Failing to specify a version may result in unexpected behaviour}.

Consider the following example: the user decides to use the \lstinline|example| module
and at that point in time, just a single version 1.2.3 is installed on the cluster.
The user loads the module using:

\begin{prompt}
%\shellcmd{module load \emph{example}}%
\end{prompt}

rather than

\begin{prompt}
%\shellcmd{module load \emph{example/1.2.3}}%
\end{prompt}

Everything works fine, up to the point where a new version of \lstinline|example| is installed, 4.5.6.
From then on, the user's load command will load the latter version, rather than
the intended one, which may lead to unexpected problems.
\ifgent % The troubleshooting section is wrapped in an ifgent, we don't want undefined references when compiling for antwerpen,...
See \autoref{sec:module-conflicts} for how to diagnose and solve module conflicts.
\fi

To list the available versions, you can use the \lstinline|module avail| command. For
example, to list the available versions of \lstinline|example|:

\begin{prompt}
%\shellcmd{module avail example/}%
example/1.2.3-intel-2018a     example/4.5.6-GCCcore-6.3.0
example/1.2.3-foss-2018a

\end{prompt}

Lets now generate a version conflict with the \lstinline|example| module, and see what happens.

\ifusinglmod
\begin{prompt}
%\shellcmd{module av example/}%
example/1.2.3
example/4.5.6
%\shellcmd{module load example/1.2.3}%
%\shellcmd{module load example/4.5.6}%
Lmod has detected the following error:  A different version of the 'example'
module is already loaded (see output of 'ml').
%\shellcmd{module swap example/4.5.6}%
\end{prompt}
\else
\begin{prompt}
%\shellcmd{module av example/}%
example/1.2.3
example/4.5.6
%\shellcmd{module load example/1.2.3}%
%\shellcmd{module load example/4.5.6}%
thirdexample/4.5.6(12):ERROR:150: Module 'example/4.5.6' conflicts with the currently loaded module(s) 'example/1.2.3'
thirdexample/4.5.6(12):ERROR:102: Tcl command execution failed: conflict example
%\shellcmd{module swap thirdexample/4.5.6}%
\end{prompt}
\fi

Note: A \lstinline|module swap| command combines the appropriate ``module unload''
and \lstinline|module load| commands.

\subsection{Get detailed info}

To get a list of all possible commands, type:

\begin{prompt}
%\shellcmd{module help}%
\end{prompt}

Or to get more information about one specific module package:

\begin{prompt}
%\shellcmd{module help example/1.2.3}%
----------- Module Specific Help for 'example/1.2.3' ---------------------------
  This is just an example - Homepage: https://example.com/
\end{prompt}

\ifusinglmod
\subsection{Save and load collections of modules}
If you have a set of modules that you need to load often,
you can save these in a \emph{collection}. This will enable you to load all the
module you need with a single command.

First, load all modules you want to include in the collections:

\begin{prompt}
%\shellcmd{module load \emph{example/1.2.3} \emph{secondexample/2.7}}%
\end{prompt}

Now store it in a collection using \lstinline|ml save|. In this example, the collection
is named \lstinline|my-collection|.

\begin{prompt}
%\shellcmd{module save \emph{my-collection}}%
\end{prompt}

Later, for example in a jobscript or a new session, you can load all these modules with
\lstinline|module restore|:

\begin{prompt}
%\shellcmd{module restore \emph{my-collection}}%
\end{prompt}

You can get a list of all your saved collections with the \lstinline|module savelist|
command.

To get a list of all modules a collection will load, you can use the \lstinline|module describe|
command:
\begin{prompt}
%\shellcmd{module describe \emph{my-collection}}%
1) example/1.2.3-intel-2018a        3) intel/2018a
2) examplelib/1.2-intel-2014b             4) secondexample/2.7-intel-2018a
\end{prompt}
\fi

To remove a collection, remove the corresponding file in \lstinline|$HOME/.lmod.d|:

\begin{prompt}
%\shellcmd{rm \$HOME/.lmod.d/my-collection}%
\end{prompt}

\section{Defining and submitting your job}

Usually, you will want to have your program running in batch mode, as opposed
to interactively as you may be accustomed to. The point is that the program
must be able to start and run without user intervention, i.e., without you
having to enter any information or to press any buttons during program
execution. All the necessary input or required options have to be specified on
the command line, or needs to be put in input or configuration files.

As an example, we will run a perl script, which you will find in the examples
subdirectory on the \hpc. When you received an account to the \hpc a
subdirectory with examples was automatically generated for you.

Remember that you have copied the contents of the HPC examples directory to
your home directory, so that you have your \strong{own personal} copy (editable
and over-writable) and that you can start using the examples. If you haven't
done so already, run these commands now:

\begin{prompt}
%\shellcmd{cd}%
%\shellcmd{cp --r \examplesdir{} \tilde{}/}%
\end{prompt}

First go to the directory with the first examples by entering the command:

\begin{prompt}
%\shellcmd{cd ~/\exampledir{}}%
\end{prompt}

Each time you want to execute a program on the \hpc you'll need 2 things:

\begin{description}
  \item[The executable] The program to execute from the end-user, together with its peripheral input files, databases and/or command options.
  \item[A batch job script], which will define the computer resource requirements of the program, the required additional software packages and which will start the actual executable.  The \hpc needs to know:
    \begin{enumerate}
      \item  the type of compute nodes;
      \item  the number of CPUs;
      \item  the amount of memory;
      \item  the expected duration of the execution time (wall time: Time as measured by a clock on the wall);
      \item  the name of the files which will contain the output (i.e., stdout) and error (i.e., stderr) messages;
      \item  what executable to start, and its arguments.
    \end{enumerate}
\end{description}

Later on, the \hpc user shall have to define (or to adapt) his/her own
job scripts. For now, all required job scripts for the
exercises are provided for you in the examples subdirectories.

List and check the contents with:

\begin{prompt}
%\shellcmd{ls -l}%
total 512
-rw-r--r-- 1 %\userid{}% 193 Sep 11 10:34 fibo.pbs
-rw-r--r-- 1 %\userid{}% 609 Sep 11 10:25 fibo.pl
\end{prompt}

In this directory you find a Perl script (named ``fibo.pl'') and a job script
(named ``fibo.pbs'').

\begin{enumerate}
\item  The Perl script calculates the first 30 Fibonacci numbers.
\item  The job script is actually a standard Unix/Linux shell script that
  contains a few extra comments at the beginning that specify directives to
  PBS.  These comments all begin with \strong{\#PBS}.
\end{enumerate}

We will first execute the program locally (i.e., on your current login-node),
so that you can see what the program does.

On the command line, you would run this using:

\begin{prompt}
%\shellcmd{./fibo.pl}%
[0] -> 0
[1] -> 1
[2] -> 1
[3] -> 2
[4] -> 3
[5] -> 5
[6] -> 8
[7] -> 13
[8] -> 21
[9] -> 34
[10] -> 55
[11] -> 89
[12] -> 144
[13] -> 233
[14] -> 377
[15] -> 610
[16] -> 987
[17] -> 1597
[18] -> 2584
[19] -> 4181
[20] -> 6765
[21] -> 10946
[22] -> 17711
[23] -> 28657
[24] -> 46368
[25] -> 75025
[26] -> 121393
[27] -> 196418
[28] -> 317811
[29] -> 514229
\end{prompt}

\underbar{Remark}: Recall that you have now executed the Perl script locally on
one of the login-nodes of the \hpc cluster.  Of course, this is not our final
intention; we want to run the script on any of the compute nodes. Also, it is
not considered as good practice, if you ``abuse'' the login-nodes for testing
your scripts and executables. It will be explained later on how you can
reserve your own compute-node (by opening an interactive session) to test
your software. But for the sake of acquiring a good understanding of what is
happening, you are pardoned for this example since these jobs require very little
computing power.

The job script contains a description of the job by specifying the command that
need to be executed on the compute node:

\examplecode{bash}{fibo.pbs}

So, jobs are submitted as scripts (bash, Perl, Python, etc.), which specify the
parameters related to the jobs such as expected runtime (walltime), e-mail
notification, etc. These parameters can also be specified on the command line.

This job script that can now be submitted to the cluster's job system for
execution, using the qsub (Queue SUBmit) command:

\begin{prompt}
%\shellcmd{qsub fibo.pbs}%
%\jobid{}%
\end{prompt}

The qsub command returns a job identifier on the HPC cluster. The important
part is the number (e.g., ``\jobnumber''); this is a unique identifier for the job
and can be used to monitor and manage your job.

Your job is now waiting in the queue for a free workernode to start on.

Go and drink some coffee \dots\ but not too long. If you get impatient you can
start reading the next section for more information on how to monitor jobs in the queue.

After your job was started, and ended, check the contents of the directory:

\begin{prompt}
%\shellcmd{ls -l}%
total 768
-rw-r--r-- 1 %\userid{} \userid{}%   44 Feb 28 13:33 fibo.pbs
-rw------- 1 %\userid{} \userid{}%    0 Feb 28 13:33 fibo.pbs.e%\jobnumber{}%
-rw------- 1 %\userid{} \userid{}% 1010 Feb 28 13:33 fibo.pbs.o%\jobnumber{}%
-rwxrwxr-x 1 %\userid{} \userid{}%  302 Feb 28 13:32 fibo.pl
\end{prompt}

Explore the contents of the 2 new files:

\begin{prompt}
%\shellcmd{more fibo.pbs.o\jobnumber{}}%
%\shellcmd{more fibo.pbs.e\jobnumber{}}%
\end{prompt}

These files are used to store the standard output and error that would
otherwise be shown in the terminal window. By default, they have the same name
as that of the PBS script, i.e., ``fibo.pbs'' as base name, followed by the
extension ``.o'' (output) and ``.e'' (error), respectively, and the job number
('\jobnumber' for this example). The error file will be empty, at least if all went
well. If not, it may contain valuable information to determine and remedy the
problem that prevented a successful run. The standard output file will contain
the results of your calculation (here, the output of the perl script)

\ifgent
\subsection{When will my job start?}
\label{subsec:priority}
In practice it's impossible to predict when your job(s) will start,
since most currently running jobs will finish before their requested walltime
expires, and new jobs by may be submitted by other users that are assigned a higher
priority than your job(s).

The \hpcInfra clusters use a fair-share sheduling policy (see \autoref{ch:hpc-policies}). There is no guarantee on when a
job will start, since it depends on a number of factors. One of these factors is
the priority of the job, which is determined by
\begin{itemize}
    \item historical use: the aim is to balance usage over users, so
        infrequent (in terms of total compute time used) users get a highter priority

    \item requested resources (amount of cores, walltime, memory, ...)

    \item time waiting in queue: queued jobs get a higher priority over time

    \item user limits: this avoids having a single user use the entire cluster.
        This means that each user can only use a part of the cluster.

\end{itemize}
Some other factors are how busy the cluster is, how many workernodes are active,
the resources (e.g. number of cores, memory) provided by each workernode, \ldots

It might be beneficial to request less resources (e.g. not requesting all cores
in a workernode), since the scheduler often finds a ``gap'' to fit the job into
more easily.

\fi

\ifgent

\subsection{Specifying the cluster on which to run}
\label{subsec:specifying-the-cluster-on-which-to-run}

To use other clusters, you can swap the \lstinline|cluster| module.
This is a special module that change what modules are available for you,
and what cluster your jobs will be queued in.

By default you are working on \defaultcluster. To switch to, e.g., \othercluster you need
to redefine the environment so you get access to all modules installed on
the \othercluster cluster, and to be able to submit jobs to the \othercluster scheduler
so your jobs will start on \othercluster instead of the default \defaultcluster cluster.

\begin{prompt}
%\shellcmd{module swap cluster/\othercluster{}}%
\end{prompt}

Note: the \othercluster modules may not work directly on the login nodes, because the
login nodes do not have the same architecture as the \othercluster cluster, they
have the same architecture as the \defaultcluster cluster however, so this is why
by default software works on the login nodes. See \autoref{sec:running-software-incompatible-with-host}
for why this is and how to fix this.

To list the available cluster modules, you can use the \lstinline|module avail cluster/| command:
\begin{prompt}
%\shellcmd{module avail cluster/}%
------------------------------------------------------------------------------------ /etc/modulefiles/vsc ------------------------------------------------------------------------------------
   cluster/delcatty (S,L)    cluster/golett (S)    cluster/phanpy (S)    cluster/skitty (S)    cluster/swalot (S)    cluster/victini (S)

  Where:
   S:  Module is Sticky, requires --force to unload or purge
   L:  Module is loaded

If you need software that is not listed, request it via https://www.ugent.be/hpc/en/support/software-installation-request
\end{prompt}

As indicated in the output above, each \lstinline|cluster| module is a so-called sticky
module, i.e. it will not be unloaded when \lstinline|module purge| (see \autoref{subsec:purging-modules}) is used.

The output of the various commands interacting with jobs (\lstinline|qsub|, \lstinline|stat|, \ldots)
all depend on which \lstinline|cluster| module is loaded.
\fi


\section{Monitoring and managing your job(s)}
\label{sec:monitoring-and-managing-your-jobs}

Using the job ID that \textit{qsub} returned, there are various ways to monitor
the status of your job, e.g.,

To get the status information on your job:

\begin{prompt}
%\shellcmd{qstat $<$jobid$>$}%
\end{prompt}

\ifgent
%don't display this because it does not work at Gent
\else
\ifbrussel
\else
  To show an estimated start time for your job (note that this may be very inaccurate,
  the margin of error on this figure can be bigger then 100\% due to a sample in a
  population of 1.)
  This command is not available on all systems.

\begin{prompt}
%\shellcmd{showstart $<$jobid$>$}%
\end{prompt}

  This is only a very rough estimate. Jobs may launch sooner than estimated if
  other jobs end faster than estimated, but may also be delayed if other
  higher-priority jobs enter the system.

  To show the status, but also the resources required by the job, with error
  messages that may prevent your job from starting:

\begin{prompt}
%\shellcmd{checkjob $<$jobid$>$}%
\end{prompt}
\fi
\fi

To show on which compute nodes your job is running, at least, when it is
running:

\begin{prompt}
%\shellcmd{qstat -n $<$jobid$>$}%
\end{prompt}

To remove a job from the queue so that it will not run, or to stop a job that
is already running.

\begin{prompt}
%\shellcmd{qdel $<$jobid$>$}%
\end{prompt}

When you have submitted several jobs (or you just forgot about the job ID), you
can retrieve the status of all your jobs that are submitted and are not yet
finished using:

% Manually indented, please keep this as is
\begin{prompt}
%\shellcmd{qstat}%
%\pbsserver{}%:
Job ID      Name    User      Time Use S Queue
----------- ------- --------- -------- - -----
%\jobnumber{}%.%\dots{}% mpi     %\userid{}% 0        Q short
\end{prompt}

Here:
\begin{description}
  \item[Job ID] the job's unique identifier
  \item[Name] the name of the job
  \item[User] the user that owns the job
  \item[Time Use] the elapsed walltime for the job
  \item[Queue] the queue the job is in
\end{description}

The state S can be any of  the following:

\begin{tabular}{|p{0.4in}|p{3.6in}|} \hline
\strong{State} & \strong{Meaning}                                             \\ \hline
\strong{Q} & The job is \strong{queued} and is waiting to start.              \\ \hline
\strong{R} & The job is currently \strong{running}.                           \\ \hline
\strong{E} & The job is currently \strong{exiting} after having run.          \\ \hline
\strong{C} & The job is \strong{completed} after having run.                  \\ \hline
\strong{H} & The job has a user or system \strong{hold} on it and will not be
  eligible to run until the hold is removed.                                  \\ \hline
\end{tabular}

User hold means that the user can remove the hold. System hold means that the system
or an administrator has put the job on hold, very likely because something is wrong with it.
Check with your helpdesk to see why this is the case.
\section{Examining the queue}

As we learned above, Moab is the software application that actually decides
when to run your job and what resources your job will run on.
\ifgent
  % Does not work in Ghent: showq is disabled as it shows information about
  % other users.
  For security reasons, it is not possible to see what other users are doing on
  the clusters. As such, the PBS \strong{qstat} command only gives information
  about your own jobs that are queued or running, ordered by \strong{JobID}.

  However, you can get some idea of the load on the clusters by specifying
  the \strong{-q} option to the \strong{qstat} command:

\begin{prompt}
%\shellcmd{qstat -q}%
server: master15.delcatty.gent.vsc

Queue            Memory CPU Time Walltime Node  Run Que Lm  State
---------------- ------ -------- -------- ----  --- --- --  -----
short              --      --    11:59:59   --    2  24 --   E R
default            --      --       --      --    0   0 --   E R
debug              --      --    00:59:59   --    1   0 --   E R
long               --      --    72:00:00   --  124 453 --   E R
                                               ----- -----
                                                 127   477
\end{prompt}

  In this example, 477 jobs are queued in the various queues whereas 127 jobs
  are effectively running.

\else
\ifbrussel
  For security reasons, it is not possible to see what other users are doing on
  the clusters. As such, the PBS \strong{qstat} command only gives information
  about your own jobs that are queued or running, ordered by \strong{JobID}.

  However, you can get some idea of the load on the clusters by specifying
  the \strong{-q} option to the \strong{qstat} command:

\begin{prompt}
%\shellcmd{qstat -q}%
server: master01.usr.hydra.brussel.vsc

Queue            Memory CPU Time Walltime Node  Run Que Lm  State
---------------- ------ -------- -------- ----  --- --- --  -----
ghostx10           --      --    120:00:0     5   0   0 --   D S
single_core       250gb    --    120:00:0     1 256   7 --   E R
ghostx8            --      --    120:00:0     9   0   0 --   E R
intel              --      --       --      --    0   0 --   D S
mpi                --      --    120:00:0   --    3   5 --   E R
smp               250gb    --    120:00:0     1 239  17 --   E R
ghostx6            --      --    120:00:0     2   0   0 --   E R
ghostx1            --      --    120:00:0     5   0   0 --   E R
ghostx11           --      --    120:00:0   --    3   2 --   E R
submission         --      --       --      --    0  24 --   E R
login              --      --    120:00:0   --    0   0 --   D S
gpu                --      --    120:00:0   --    0   0 --   E R
himem              --      --    120:00:0     1   0   0 --   E R
                                               ----- -----
                                                 501    55
\end{prompt}

  In this example, 55 jobs are queued in the various queues whereas 501 jobs
  are effectively running.

\else
  You can look at
  the queue by using the PBS \strong{qstat} command or the Moab
  \strong{showq} command. By default, \strong{qstat} will display the queue
  ordered by \strong{JobID}, whereas \strong{showq} will display jobs grouped by
  their state (``running'', ``idle'', or ``hold'') then ordered by priority.
  Therefore, \strong{showq} is often more useful.
  Note however that at some VSC-sites, these commands show only your jobs or may
  be even disabled to not reveal what other users are doing.

  The \strong{showq} command displays information about active (``running''),
  eligible (``idle''), blocked (``hold''), and/or recently completed jobs. To get
  a summary:

\begin{prompt}
%\shellcmd{showq -s}%
active jobs: 163
eligible jobs: 133
blocked jobs: 243
Total jobs:  539
\end{prompt}

\fi
\fi

\ifantwerpen
And to get the full detail of all the jobs, which are in the system:

\begin{prompt}
%\shellcmd{showq}%
active jobs------------------------
JOBID     USERNAME  STATE PROCS REMAINING          STARTTIME
428024    vsc20167  Running   8   2:57:32  Mon Sep  2 14:55:05
%\dots{}%
153 active jobs 1307 of 3360 processors in use by local jobs (38.90%\%%)
153 of 168 nodes active      (91.07%\%%)

eligible jobs----------------------
JOBID     USERNAME  STATE PROCS   WCLIMIT            QUEUETIME
442604    vsc20167   Idle  48  7:00:00:00  Sun Sep 22 16:39:13
442605    vsc20167   Idle  48  7:00:00:00  Sun Sep 22 16:46:22
%\dots{}%

135 eligible jobs

blocked jobs-----------------------
JOBID   USERNAME     STATE PROCS WCLIMIT            QUEUETIME
441237  vsc20167      Idle   8 3:00:00:00 Thu Sep 19 15:53:10
442536  vsc20167  UserHold  40 3:00:00:00 Sun Sep 22 00:14:22
%\dots{}%
252 blocked jobs
Total jobs:  540
\end{prompt}
\fi

\ifgent
  %% Again, don't show because showq does not work
\else
\ifbrussel
\else
  There are 3 categories, the \strong{active}, \strong{eligible} and \strong{blocked} jobs.

  \begin{description}
    \item[Active jobs] are jobs that are running or starting and that consume computer resources. The amount of time remaining (w.r.t.\ walltime, sorted to earliest completion time) and the start time are displayed. This will give you an idea about the foreseen completion time. These jobs could be in a number of states:

    \begin{description}
      \item[Started] attempting to start, performing pre-start tasks
      \item[Running] currently executing the user application
      \item[Suspended] has been suspended by scheduler or admin (still in place
        on the allocated resources, not executing)
      \item[Cancelling] has been cancelled, in process of cleaning up
    \end{description}

    \item[Eligible jobs] are jobs that are waiting in the queues and are
      considered eligible for both scheduling and backfilling.  They are all in
      the idle job state and do not violate any fairness policies or do not have
      any job holds in place. The requested walltime is displayed, and the list
      is ordered by job priority.
    \item[Blocked jobs] are jobs that are ineligible to be run or queued.  These
      jobs could be in a number of states for the following reasons:

    \begin{description}
      \item[Idle] when the job violates a fairness policy
      \item[Userhold] or systemhold when it is user or administrative hold
      \item[Batchhold] when the requested resources are not available or the resource manager has repeatedly failed to start the job
      \item[Deferred] when a temporary hold when the job has been unable to start after a specified number of attempts
      \item[Notqueued] when scheduling daemon is unavailable
    \end{description}
  \end{description}
\fi
\fi

\section{Specifying job requirements}

Without giving more information about your job upon submitting it with
\strong{qsub}, default values will be assumed that are almost never appropriate
for real jobs.

It is important to estimate the resources you need to successfully run your
program, such as the amount of time the job will require, the amount of memory
it needs, the number of CPUs it will run on, etc. This may take some work, but
it is necessary to ensure your jobs will run properly.

\subsection{Generic resource requirements}
\label{subsec:generic-resource-requirements}

The \strong{qsub} command takes several options to specify the requirements, of which we
list the most commonly used ones below. \\

\begin{prompt}
%\shellcmd{qsub -l walltime=2:30:00}%
\end{prompt}

For the simplest cases, only the amount of maximum estimated execution time
(called ``walltime'') is really important. Here, the job requests
2 hours, 30 minutes. As soon as the job exceeds the requested walltime,
it will be ``killed'' (terminated) by the job scheduler.  There is no
harm if you \emph{slightly} overestimate the maximum execution time.
If you omit this option, the queue manager will not complain but use a
default value (one hour on most clusters).

\begin{prompt}
%\shellcmd{qsub -l mem=4gb}%
\end{prompt}

The job requests 4 GB of RAM memory. As soon as the job tries to use more memory,
it will be ``killed'' (terminated) by the job scheduler.  There is no
harm if you \emph{slightly} overestimate the requested memory. \\
 \\

\begin{prompt}
%\shellcmd{qsub -l nodes=5:ppn=2}%
\end{prompt}

The job requests 5 compute nodes with two cores on each node (ppn stands for
``processors per node'', where "processors" here actually means "CPU cores"). \\

\begin{prompt}
%\shellcmd{qsub -l nodes=1:westmere}%
\end{prompt}

The job requests just one node, but it should have an Intel Westmere processor.
A list with site-specific properties can be found in the next section or in the
User Portal
(``Available hardware''-section)\footnote{URL: \url{https://www.vscentrum.be/infrastructure/hardware}}
of the VSC website.

These options can either be specified on the command line, e.g.

\begin{prompt}
%\shellcmd{qsub -l nodes=1:ppn=1,mem=2gb fibo.pbs}%
\end{prompt}

or in the job script itself using the \#PBS-directive, so ``fibo.pbs'' could be modified to:

\begin{code}{bash}
#!/bin/bash -l
#PBS -l nodes=1:ppn=1
#PBS -l mem=2gb
cd $PBS_O_WORKDIR
./fibo.pl
\end{code}

Note that the resources requested on the command line will override those
specified in the PBS file.

\subsection{Available job categories (TORQUE queues)}

In order to guarantee a fair share access to the computer resources to all
users, only a limited number of jobs with certain walltimes are possible per
user.

We therefore classify the submitted jobs in categories (confusingly also called
queues), depending on the their walltime specification.  A user is allowed to
run up to a certain maximum number of jobs in each of these walltime
categories.

The currently defined walltime categories for the \hpc
are:

\inputsite{queue-table}

% I suggest to strike this comment. We will only instill anticipation in our users
%\underbar{Remark:} As the infrastructure of the \hpc is constantly expanding
%and evolving, it can be anticipated that also the limits of the categories can
%be changed over time.

\iffalse
% TODO : already adapt this text according to the new common user experience rules, and make it general....
When a user submits a job with a walltime of 6 days, the queue manager will
put the job in the walltime category 3 days--7 days.  The user can submit up to 400 jobs with
this high walltimes (queueable = 400) on hopper, but only 50 of those jobs will be eligible
for execution (runnable=50) at the same time.  A detailed description of the
fair-share mechanisms will follow in \autoref{ch:hpc-policies}. For longer running jobs,
\emph{checkpointing}
%(See \autoref{ch:checkpointing}), https://github.com/hpcugent/vsc_user_docs/issues/205
is necessary.
\fi

\iffalse
Apart from specifying the \emph{walltime}, you can also explicitly define the
queue you're submitting your job to.

To specify the queue, add:

\begin{prompt}
-q queuename
\end{prompt}
to the qsub command line, or
\begin{code}{bash}
#PBS -q queuename
\end{code}

to the job script, where \emph{queuename} is one of the possible queues shown
above.

A maximum \emph{walltime} is associated with each queue.

The queue category logic is:

\begin{tabular}{|p{0.9in}|p{2.0in}|p{2.0in}|} \hline
                                     & \strong{No walltime specified}                                          & \strong{Walltime specified} \\ \hline
\strong{No queue \newline specified} & The job is submitted in the qshort queue with a walltime of 1 hour.     & The job is submitted in the proper queue in accordance with the given walltime. \\ \hline
\strong{Queue \newline specified}    & The job is submitted in your specified
  queue with q walltime of 1 hour. & The job is submitted in the specified
  queue with the specified walltime. If the specified \textit{walltime} is
  larger than the maximal \textit{walltime} of the requested queue, the job
  cannot be submitted. \\ \hline
\end{tabular}

\underbar{Remark:} It is highly recommended to specify a walltime at all times
in all your job scripts. Only for some short test-runs, a walltime
specification could be omitted.

When a user tries to submit more jobs in a certain walltime category than the
maximum number of queable jobs, the submission will fail. The scheduler will
also enforce that no more than the maximum of runnable jobs per category are
being executed at the same time.

To get basic information about the queues, the command \strong{qstat -q} is used:

\inputsite{qstat-q-output}

The number of jobs currently running in the queue is shown in the Run column,
whereas the number of jobs waiting to get started is shown in the Queue column.
The maximum walltime that is accepted by the queue is shown in the Walltime
column.

To obtain more detailed information on the queues the following command can be
used:

\begin{prompt}
%\shellcmd{qstat -f -Q <queuename>}%
\end{prompt}

This will list additional restrictions such as the maximum walltime of the jobs
and the maximum number of jobs that a user can have in that queue.
\fi

\subsection{Node-specific properties}

The following table contains some node-specific properties that can be used to
make sure the job will run on nodes with a specific CPU or interconnect. Note
that these properties may vary over the different VSC sites.

%TODO insert specs for other sites
\ifantwerpen
\begin{tabular}{|p{0.7in}|p{5.3in}|} \hline
\strong{Property} & \strong{Explanation}                                                        \\ \hline
ivybridge         & only use Intel processors from the Ivy Bridge family (26xx-v2, hopper-only) \\ \hline
broadwell         & only use Intel processors from the Broadwell family (26xx-v4, leibniz-only) \\ \hline
mem256            & only use nodes with 256GB of RAM (hopper and leibniz) \\ \hline
\end{tabular}

Since both hopper and leibniz are homogeneous with respect to processor architecture, the CPU architecture properties
are not really needed and only defined for compatibility with other VSC clusters.
\fi
\ifbrussel
\begin{tabular}{|p{0.7in}|p{5.3in}|} \hline
\strong{Property} & \strong{Explanation}                                        \\ \hline
shanghai          & only use AMD Shanghai processors (AMD 2378) \\ \hline
magnycours        & only use AMD Magnucours processors (AMD 6134) \\ \hline
interlagos        & only use AMD Interlagos processors (AMD 6272) \\ \hline
barcelona         & only use AMD Shanghai and Magnycours processors \\ \hline
amd               & only use AMD processors \\ \hline
ivybridge         & only use Intel Ivy Bridge processors (E5-2680-v2) \\ \hline
intel             & only use Intel processors \\ \hline
gpgpu             & only use nodes with General Purpose GPUs (GPGPUs) \\ \hline
k20x              & only use nodes with NVIDIA Tesla K20x GPGPUs \\ \hline
xeonphi           & only use nodes with Xeon Phi co-processors \\ \hline
phi5110p          & only use nodes with Xeon Phi 5110P co-processors \\ \hline
\end{tabular}
\fi

To get a list of all properties defined for all nodes, enter

\begin{prompt}
%\shellcmd{pbsnodes}%
\end{prompt}

This list will also contain properties referring to, e.g., network components,
rack number, etc.

\section{Job output and error files}

At some point your job finishes, so you may no longer see the job ID in the
list of jobs when you run \emph{qstat} (since it will only be listed for a few
minutes after completion with state ``C''). After your job finishes, you should
see the standard output and error of your job in two files, located by default
in the directory where you issued the \emph{qsub} command.


When you navigate to that directory and list its contents, you should see them:

\begin{prompt}
%\shellcmd{ls -l}%
total 1024
-rw-r--r-- 1 %\userid{}%  609 Sep 11 10:54 fibo.pl
-rw-r--r-- 1 %\userid{}%   68 Sep 11 10:53 fibo.pbs
-rw------- 1 %\userid{}%   52 Sep 11 11:03 fibo.pbs.e%\jobnumber{}%
-rw------- 1 %\userid{}% 1307 Sep 11 11:03 fibo.pbs.o%\jobnumber{}%
\end{prompt}

In our case, our job has created both output (`fibo.pbs.\strong{o}\jobnumber') and
error files (`fibo.pbs.\strong{e}\jobnumber') containing info written to
\emph{stdout} and \emph{stderr} respectively.

Inspect the generated output and error files:

\begin{prompt}
%\shellcmd{cat fibo.pbs.o\jobnumber{}}%
%\dots{}%
%\shellcmd{cat fibo.pbs.e\jobnumber{}}%
%\dots{}%
\end{prompt}

\section{E-mail notifications}

\ifgent
  %do not display this, since at least in Gent it doesn't behave that way
\else
  \subsection{Upon job failure}

  Whenever a job fails, an e-mail will be sent to the e-mail address that's
  connected to your $<$vsc-account$>$. This is the e-mail address that is linked
  to the university account, which was used during the registration process.

  You can force a job to fail by specifying an unrealistic wall-time for the
  previous example.  Lets give the ``\emph{fibo.pbs}'' job just one second to
  complete:

\begin{prompt}
%\shellcmd{qsub -l walltime=0:00:01 fibo.pbs}%
\end{prompt}

  Now, lets hope that the \hpc did not manage to run the job within one second,
  and you will get an e-mail informing you about this error.

  \begin{flattext}
  PBS Job Id: %\jobid%
  Job Name:   fibo.pbs
  Exec host:  %\computenode%/0
  Aborted by PBS Server
  Job exceeded some resource limit (walltime, mem, etc.). Job was aborted.
  See Administrator for help
  \end{flattext}
\fi

\subsection{Generate your own e-mail notifications}

You can instruct the \hpc to send an e-mail to your e-mail address whenever a
job \strong{b}egins, \strong{e}nds and/or \strong{a}borts, by adding the
following lines to the job script ``fibo.pbs'':

\begin{code}{bash}
#PBS -m b
#PBS -m e
#PBS -m a
#PBS -M <your e-mail address>
\end{code}
or
\begin{code}{bash}
#PBS -m abe
#PBS -M <your e-mail address>
\end{code}

These options can also be specified on the command line.  Try it and see what
happens:

\begin{prompt}
%\shellcmd{qsub -m abe -M $<$your e-mail address$>$ fibo.pbs}%
\end{prompt}

You don't have to specify the e-mail address. The system will
use the e-mail address which is connected to your VSC account.
