\chapter{Compiling and testing your software on the UA-HPC}

All nodes in the UA-HPC cluster are running the ``Scientific Linux'' (SL) Operating system, which is a specific version of RedHat-Linux. This means that all the software programs (executable) that the end-user wants to run on the UA-HPC, first must be compiled for the SL Operating System.  It also means that you first have to install all the required external software packages on the UA-HPC.

Most commonly used compilers are already pre-installed on the UA-HPC, and can be used straight away. Also many popular external software packages, which are regularly used in the scientific community, are also pre-installed.

\section{Check the pre-installed software on the UA-HPC}

In order to check all the available modules and their version numbers, which are pre-installed on the UA-HPC, enter:
\begin{prompt}
$ %\textbf{module av 2$>$\&1 \textbar  more}%
-------------------- /apps/local/turing/harpertown/modules ---------------------
ADF/2012.01
DALTON/2011.v0
FASTX/0.0.13
FILTLAN/1.0a-ictce-3.2.2.u2
FastQC/0.10.1
GROMACS/4.5.1-ictce-3.2.2.u2
GenomeAnalysisTK/2.2-3
\dots
\end{prompt}

``module av'' is an abbreviation for ``Module available''.

Or when you want to check whether some specific software, some compiler or some application (e.g., Molpro) is installed on the UA-HPC:
\begin{prompt}
$ %\textbf{module av 2$>$\&1 \textbar  grep -i -e "molpro"}%
Molpro/2009.1-OpenMPI-1.4.1
Molpro/2009.1-ga-4.3.1-TCGMSG-MPI-ictce-3.2.2.u2
Molpro/2009.1-ga-4.3.2-TCGMSG-MPI-ictce-3.2.2.u2
Molpro/2009.1-ictce-3.2.2.013.u4
Molpro/2010.1-MPI-ictce-3.2.2.u2
Molpro/2010.1-ga-4.3.2-TCGMSG-MPI-ictce-3.2.2.u2
\end{prompt}

As you are not aware of the capitals letters in the module name, we looked for a case-insensitive name with the ``-i'' option.

When your required application is not available on the UA-HPC, please contact any UA-HPC-staff member. Be aware of potential ``License Costs''.  ``Open Source'' software is often preferred.

\section{Porting your code}

To \textbf{port} a software-program is to translate it from the operating system in which it was developed (e.g., Windows 7) to another operating system (e.g., Scientific Linux on our UA-HPC) so that it can be used there. Porting implies some degree of effort, but not nearly as much as redeveloping the program in the new environment.  It all depends on how ``portable'' you wrote your code.

In the simplest case the file or files may simply be copied from one machine to the other. However, in many cases the software is installed on a computer in a way, which depends upon its detailed hardware, software, and setup, with device drivers for particular devices, using installed operating system and supporting software components, and using different directories.

In some cases software, usually described as ``portable software'' is specifically designed to run on different computers with compatible operating systems and processors without any machine-dependent installation; it is sufficient to transfer specified directories and their contents. Hardware- and software-specific information is often stored in configuration files in specified locations (e.g., the registry on machines running MS Windows).

Software, which is not portable in this sense, will have to be transferred with modifications to support the environment on the destination machine.

Whilst programming, it would be wise to stick to certain standards (e.g., ISO/ANSI/POSIX).  This will ease the porting of your code to other platforms.

Porting your code to the SL-Linux platform is the responsibility of the end-user.

\section{Compiling and building on the UA-HPC}

Compiling refers to the process of translating code written in some programming language, e.g. Fortran, C, or C++, to machine code. Building is similar, but includes gluing together the machine code resulting from different source files into an executable (or library). The text below guides you through some basic problems typical for small software projects. For larger projects it is more appropriate to use makefiles or even an advanced build system like CMake.

All the UA-HPC nodes run the same version of the Operating System, i.e. Scientific Linux. So, it is sufficient to compile your program on any compute node.  Once you have generated an executable with your compiler, this executable should be able to run on any other compute-node.

A typical process looks like:

\begin{enumerate}
\item  Copy your software to the login-node of the UA-HPC;
\item  Start an interactive session on a compute node;
\item  Compile it;
\item  Test it locally;
\item  Generate your job-scripts;
\item  Test it on the UA-HPC;
\item  Run it (in parallel);
\end{enumerate}

We assume you've copied your software to the UA-HPC. The next step is to request your private compute node.
\begin{prompt}
$ %\textbf{qsub -I }%
qsub: waiting for job 443551.master1.turing.antwerpen.vsc to start
\end{prompt}

\subsection{Compiling a sequential program in C}

Go to the examples for Chapter 9 and load the GCC modules:
\begin{prompt}
$ %\textbf{cd \~/examples/Chapter10\_Compile}%
$ %\textbf{module load GCC}%
\end{prompt}

We now list the directory and explore the contents of the ``\textit{hello.c}'' program:
\begin{prompt}
$ %\textbf{ls -l}%
total 512
-rw-r--r-- 1 vsc20167 214 Sep 16 09:42 hello.c
-rw-r--r-- 1 vsc20167 130 Sep 16 11:39 hello.pbs*
-rw-r--r-- 1 vsc20167 359 Sep 16 13:55 mpi\_hello.c
-rw-r--r-- 1 vsc20167 304 Sep 16 13:55 mpi\_hello.pbs
$ %\textbf{more hello.c}%
\#include "stdio.h"
int main( int argc, char *argv[] )
\{
  int i;
  for (i=0; i$<$500; i++)
  \{
    printf("Hello \#\%d\textbackslash n", i);
    fflush(stdout);
    sleep(1);
  \}
\}
\end{prompt}

The ``hello.c'' program is a simple source file, written in C. It'll print 500 times ``Hello \#$<$num$>$'', and waits one second between 2 printouts.

We first need to compile this C-file into an executable with the gcc-compiler.

First, check the command line options for ``\textit{gcc'' (GNU C-Compiler)}, then we compile and list the contents of the directory again:

\begin{prompt}
$ %\textbf{gcc --help}%
$ %\textbf{gcc -o hello hello.c}%
$ %\textbf{ls -l }%
total 512
-rwxrwxr-x 1 vsc20167 7116 Sep 16 11:43 hello*
-rw-r--r-- 1 vsc20167  214 Sep 16 09:42 hello.c
-rwxr-xr-x 1 vsc20167  130 Sep 16 11:39 hello.pbs*
\end{prompt}

A new file ``hello'' has been created. Note that this file has ``execute'' rights, i.e. it is an executable. More often than not, calling gcc -- or any other compiler for that matter -- will provide you with a list of errors and warnings referring to mistakes the programmer made, such as typos, syntax errors. You will have to correct them first in order to make the code compile. Warnings pinpoint less crucial issues that may relate to performance problems, using unsafe or obsolete language features, etc. It is good practice to remove all warnings from a compilation process, even if they seem unimportant so that a code change that produces a warning does not go unnoticed.

Let's test this program on the local compute node, which is at your disposal after the ``qsub --I'' command:

\begin{prompt}
$ %\textbf{./hello}%
Hello \#0
Hello \#1
Hello \#2
Hello \#3
Hello \#4
\dots
\end{prompt}


It seems to work, now run it on the UA-HPC:
\begin{prompt}
$ %\textbf{qsub hello.pbs}%
\end{prompt}

\subsection{Compiling a parallel program in C/MPI}

Stay in the examples directory for Chapter 9:
\begin{prompt}
$ %\textbf{cd \~/examples/Chapter09\_Compile}%
\end{prompt}

List the directory and explore the contents of the ``\textit{mpi\_hello.c}'' program:
\begin{prompt}
$ %\textbf{ls -l}%
total 512
total 512
-rw-r--r-- 1 vsc20167 214 Sep 16 09:42 hello.c
-rw-r--r-- 1 vsc20167 130 Sep 16 11:39 hello.pbs*
-rw-r--r-- 1 vsc20167 359 Sep 16 13:55 mpihello.c
-rw-r--r-- 1 vsc20167 304 Sep 16 13:55 mpihello.pbs
$ %\textbf{more mpihello.c}%
\dots
\end{prompt}

The ``mpi\_hello.c'' program is a simple source file, written in C with MPI library calls.

We first need to compile this file into an executable with the \textit{mpicc}-compiler. The mpicc compiler is simply a MPI-aware wrapper around the gcc compiler. It is available from the impi modules.

First, search the relevant MPI modules, and load the most recent.
\begin{prompt}
$ %\textbf{module av 2$>$\&1 \textbar  grep -i mpi}%
impi/3.2.1.009
impi/3.2.2.006
impi/4.0.0.025
impi/4.0.0.027
impi/4.0.0.028
impi/4.0.1.007
impi/4.1.0.027
impi/4.1.1.036
\dots
$ %\textbf{module load impi}%
\end{prompt}

Then, check the command line options for ``\textit{mpicc'' (GNU C-Compiler with MPI extensions)}, then we compile and list the contents of the directory again:

\begin{prompt}
$ %\textbf{mpicc --help}%
$ %\textbf{mpicc -o  mpihello  mpihello.c}%
$ %\textbf{ls -l}%
\end{prompt}

A new file ``hello'' has been created. Note that this program has ``execute'' rights.

Let's test this program on the ``login''-node first:
\begin{prompt}
$ %\textbf{./mpihello}%
Hello World from Node 0.
\end{prompt}

It seems to work, now run it on the UA-HPC:
\begin{prompt}
$ %\textbf{qsub mpihello.pbs}%
\end{prompt}

\subsection{Compiling a parallel program in INTEL Cluster Studio}

We will now compile the same program, but using the INTEL Cluster Studio compilers. We stay in the examples directory for Chapter 9:

\begin{prompt}
$ %\textbf{cd \~/examples/Chapter09\_Compile}%
\end{prompt}

We will compile this C/MPI -file into an executable with the INTEL Cluster Studio compiler. First, clear the modules (purge) and then load the latest ``ictce'' (Intel Cluster Toolkit Compiler Edition) module:
\begin{prompt}
$ %\textbf{module purge}%
$ %\textbf{module load ictce}%
\end{prompt}

Then, compile and list the contents of the directory again. The ictce equivalent of mpicc is mpiicc.
\begin{prompt}
$ %\textbf{mpiicc -o mpihello mpihello.c}%
$ %\textbf{ls -l}%
\end{prompt}

Note that the old ``mpihello'' file has been overwritten.
Let's test this program on the ``login''-node first:
\begin{prompt}
$ %\textbf{./mpihello}%
Hello World from Node 0.
\end{prompt}

It seems to work, now run it on the UA-HPC:
\begin{prompt}
$ %\textbf{qsub mpihello.pbs}%
\end{prompt}

Note: The UA only has a license for the Intel Cluster Studio for 5 concurrent users. As such, it might happen that you have to wait a few minutes before a floating license becomes available for your use.

Note: The Intel Cluster Studio contains equivalent compilers for all GNU compilers. Hereafter the overview for C, C++ and Fortran compilers.

\begin{tabular}{|p{0.7in}|p{0.5in}|p{0.3in}|p{0.8in}|p{0.3in}|p{0.5in}|p{0.8in}|} \hline
\textbf{} & \multicolumn{3}{|p{1.6in}|}{\textbf{Sequential Program}} & \multicolumn{3}{|p{1.6in}|}{\textbf{Parallel Program (with MPI)}} \\ \hline
\textbf{} & \multicolumn{2}{|p{0.8in}|}{\textbf{GNU}} & \textbf{Intel} & \multicolumn{2}{|p{0.8in}|}{\textbf{GNU}} & \textbf{Intel} \\ \hline
\textbf{C} & \multicolumn{2}{|p{0.8in}|}{gcc} & icc & \multicolumn{2}{|p{0.8in}|}{mpicc} & mpiicc \\ \hline
\textbf{C++} & \multicolumn{2}{|p{0.8in}|}{g++} & icpc & \multicolumn{2}{|p{0.8in}|}{mpicxx} & mpiicpc \\ \hline
\textbf{Fortran} & \multicolumn{2}{|p{0.8in}|}{gfortran} & ifort & \multicolumn{2}{|p{0.8in}|}{mpif90} & mpiifort \\ \hline
\end{tabular}
